{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wdJroFj8D8H5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0e1949-bd53-4aad-be27-6ae8c9d243c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.9.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.11)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.5.2 (from gradio)\n",
            "  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.42.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gTTS) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gTTS) (8.1.7)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.151.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.25.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.24)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.20.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.20.1+cu121)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.10.0.84)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.13.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.24.0)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.6)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.66.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf (from google-generativeai)\n",
            "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Downloading gradio-5.9.0-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (912 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.2/912.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading python_bidi-0.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.8/286.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=1ef1022a5dbae00cce4a710f3dc0be7ac815733b412d6b1cf85b0bd9a670a584\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: python-bidi, pypika, pydub, pyclipper, monotonic, durationpy, XlsxWriter, uvloop, uvicorn, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, python-docx, pytesseract, pyproject_hooks, PyPDF2, protobuf, overrides, opentelemetry-util-http, ninja, mmh3, markupsafe, humanfriendly, httptools, ffmpy, chroma-hnswlib, bcrypt, backoff, asgiref, aiofiles, watchfiles, tiktoken, starlette, python-pptx, posthog, opentelemetry-proto, opentelemetry-api, gTTS, coloredlogs, build, safehttpx, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, gradio-client, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, gradio, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, easyocr, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.28.2\n",
            "    Uninstalling opentelemetry-api-1.28.2:\n",
            "      Successfully uninstalled opentelemetry-api-1.28.2\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.49b2\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.49b2:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.49b2\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.28.2\n",
            "    Uninstalling opentelemetry-sdk-1.28.2:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.28.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyPDF2-3.0.1 XlsxWriter-3.2.0 aiofiles-23.2.1 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 easyocr-1.7.2 fastapi-0.115.6 ffmpy-0.4.0 gTTS-2.5.4 gradio-5.9.0 gradio-client-1.5.2 httptools-0.6.4 humanfriendly-10.0 kubernetes-31.0.0 markupsafe-2.1.5 mmh3-5.0.1 monotonic-1.6 ninja-1.11.1.3 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.7.4 protobuf-5.29.1 pyclipper-1.3.0.post6 pydub-0.25.1 pypika-0.48.9 pyproject_hooks-1.2.0 pytesseract-0.3.13 python-bidi-0.6.3 python-docx-1.1.2 python-dotenv-1.0.1 python-multipart-0.0.19 python-pptx-1.0.2 ruff-0.8.3 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tiktoken-0.8.0 tomlkit-0.13.2 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.3\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install gradio gTTS PyPDF2 python-docx pandas python-pptx google-generativeai langchain chromadb tiktoken pytesseract easyocr\n",
        "\n",
        "# Standard library imports\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import tempfile\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "\n",
        "# Third-party imports\n",
        "import cv2\n",
        "import docx\n",
        "import easyocr\n",
        "import google.generativeai as genai\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import pytesseract\n",
        "from gtts import gTTS\n",
        "from pptx import Presentation\n",
        "\n",
        "# Configure Google API\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyCufC37xgbXAS-gJPccd29YBz2pP1jzH2I\"\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Any,Optional\n",
        "from dataclasses import dataclass, field"
      ],
      "metadata": {
        "id": "T23wOr4-TCIW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio User Interface (Citation Task Working fine for PDF,PPTX,CSV files)"
      ],
      "metadata": {
        "id": "TCn_ZONrxifp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DOCX File Citataion(Working for Citation sources and migrating to the source guide but not highlighting text response)"
      ],
      "metadata": {
        "id": "hNk4hMTMi8GX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import shutil\n",
        "\n",
        "class FileInfo:\n",
        "    def __init__(self, path: str):\n",
        "        self.path = path\n",
        "        self.name = os.path.basename(path)\n",
        "        self.size = os.path.getsize(path)\n",
        "\n",
        "def extract_text_from_pdf(file_path: str) -> str:\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            text_with_pages = []\n",
        "            for page_num, page in enumerate(reader.pages, 1):\n",
        "                page_text = page.extract_text()\n",
        "                text_with_pages.append(f'<div id=\"page{page_num}\" class=\"page-content\">[PAGE {page_num}]\\n{page_text}</div>')\n",
        "            text = '\\n'.join(text_with_pages)\n",
        "            text_file_path = f\"{os.path.splitext(file_path)[0]}_text.txt\"\n",
        "            with open(text_file_path, 'w', encoding='utf-8') as txt_file:\n",
        "                txt_file.write(text)\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        return f\"Error processing PDF: {str(e)}\"\n",
        "\n",
        "def extract_text_from_pptx(file_path: str) -> str:\n",
        "    try:\n",
        "        prs = Presentation(file_path)\n",
        "        text = '\\n'.join(\n",
        "            f\"[SLIDE {i+1}]\\n{shape.text}\"\n",
        "            for i, slide in enumerate(prs.slides)\n",
        "            for shape in slide.shapes\n",
        "            if hasattr(shape, \"text\")\n",
        "        )\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error processing PPTX: {str(e)}\"\n",
        "\n",
        "def extract_text_from_docx(file_path: str) -> str:\n",
        "    doc = docx.Document(file_path)\n",
        "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "def extract_text_from_csv_excel(file_path: str) -> str:\n",
        "    try:\n",
        "        ext = os.path.splitext(file_path)[1].lower()\n",
        "        if ext == '.csv':\n",
        "            df = pd.read_csv(file_path)\n",
        "            rows_with_markers = []\n",
        "            for idx, row in df.iterrows():\n",
        "                row_text = [f\"[ROW {idx+1}]\"]\n",
        "                for col in df.columns:\n",
        "                    row_text.append(f\"{col}: {row[col]}\")\n",
        "                rows_with_markers.append('\\n'.join(row_text))\n",
        "            full_data = '\\n\\n'.join(rows_with_markers)\n",
        "            text = (\n",
        "                f\"File Summary:\\n\"\n",
        "                f\"Total Rows: {len(df)}\\n\"\n",
        "                f\"Columns: {', '.join(df.columns)}\\n\\n\"\n",
        "                f\"Complete Data:\\n{full_data}\\n\\n\"\n",
        "                f\"Basic Statistics:\\n{df.describe().to_string()}\\n\\n\"\n",
        "                f\"Searchable Fields: {', '.join(df.columns)}\\n\"\n",
        "            )\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        return f\"Error processing CSV/Excel: {str(e)}\"\n",
        "\n",
        "def extract_text_from_excel(file_path: str) -> str:\n",
        "    try:\n",
        "        ext = os.path.splitext(file_path)[1].lower()\n",
        "        if ext == '.xlsx':\n",
        "            df = pd.read_excel(file_path)\n",
        "            rows_with_markers = []\n",
        "            for idx, row in df.iterrows():\n",
        "                row_marker = f\"[ROW {idx+1}]\"\n",
        "                row_text = [row_marker]\n",
        "                for col in df.columns:\n",
        "                    row_text.append(f\"{col}: {row[col]}\")\n",
        "                rows_with_markers.append('\\n'.join(row_text))\n",
        "            full_data = '\\n\\n'.join(rows_with_markers)\n",
        "            text = (\n",
        "                f\"File Summary:\\n\"\n",
        "                f\"Total Rows: {len(df)}\\n\"\n",
        "                f\"Columns: {', '.join(df.columns)}\\n\\n\"\n",
        "                f\"Complete Data:\\n{full_data}\\n\\n\"\n",
        "                f\"Basic Statistics:\\n{df.describe().to_string()}\\n\\n\"\n",
        "                f\"Searchable Fields: {', '.join(df.columns)}\\n\"\n",
        "            )\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        return f\"Error processing CSV/Excel: {str(e)}\"\n",
        "\n",
        "def preprocess_image(image_path: str) -> np.ndarray:\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "    coords = np.column_stack(np.where(gray > 0))\n",
        "    angle = cv2.minAreaRect(coords)[-1]\n",
        "    if angle < -45:\n",
        "        angle = -(90 + angle)\n",
        "    else:\n",
        "        angle = -angle\n",
        "    (h, w) = gray.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    rotated = cv2.warpAffine(gray, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
        "    return rotated\n",
        "\n",
        "def extract_text_from_image(image_path: str, method: str = 'pytesseract') -> str:\n",
        "    try:\n",
        "        if method == 'pytesseract':\n",
        "            preprocessed_img = preprocess_image(image_path)\n",
        "            text = pytesseract.image_to_string(preprocessed_img)\n",
        "        elif method == 'easyocr':\n",
        "            reader = easyocr.Reader(['en'])\n",
        "            results = reader.readtext(image_path)\n",
        "            text = ' '.join([result[1] for result in results])\n",
        "        else:\n",
        "            raise ValueError(\"Invalid OCR method. Choose 'pytesseract' or 'easyocr'.\")\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting text from image: {str(e)}\"\n",
        "\n",
        "def extract_text_from_charts_and_graphs(image_path: str) -> Dict:\n",
        "    try:\n",
        "        img = cv2.imread(image_path)\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        pytesseract_text = extract_text_from_image(image_path, 'pytesseract')\n",
        "        easyocr_text = extract_text_from_image(image_path, 'easyocr')\n",
        "        chart_info = {\n",
        "            'text_pytesseract': pytesseract_text,\n",
        "            'text_easyocr': easyocr_text,\n",
        "            'contour_count': len(contours),\n",
        "            'approximate_chart_type': _identify_chart_type(img)\n",
        "        }\n",
        "        return chart_info\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error analyzing chart: {str(e)}\"}\n",
        "\n",
        "def _identify_chart_type(img: np.ndarray) -> str:\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    edges = cv2.Canny(gray, 50, 150)\n",
        "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    aspect_ratios = [cv2.contourArea(cnt) / (cv2.arcLength(cnt, True) ** 2) for cnt in contours]\n",
        "    if len(contours) > 10:\n",
        "        if np.mean(aspect_ratios) < 0.1:\n",
        "            return \"Bar Chart\"\n",
        "        elif np.mean(aspect_ratios) > 0.5:\n",
        "            return \"Pie Chart\"\n",
        "        else:\n",
        "            return \"Complex Chart/Graph\"\n",
        "    return \"Simple Image/Graphic\"\n",
        "\n",
        "def process_file(file_path: str) -> Tuple[str, str]:\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    file_name = os.path.basename(file_path)\n",
        "    try:\n",
        "        if ext == '.pdf':\n",
        "            return extract_text_from_pdf(file_path)\n",
        "        elif ext == '.docx':\n",
        "            return extract_text_from_docx(file_path)\n",
        "        elif ext == '.pptx':\n",
        "            return extract_text_from_pptx(file_path)\n",
        "        elif ext == '.csv':\n",
        "            return extract_text_from_csv_excel(file_path)\n",
        "        elif ext == '.xlsx':\n",
        "            return extract_text_from_excel(file_path)\n",
        "        elif ext == '.txt':\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return f.read()\n",
        "        elif ext in ['.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.webp']:\n",
        "            image_text = extract_text_from_image(file_path, 'pytesseract')\n",
        "            if not image_text.strip():\n",
        "                image_text = extract_text_from_image(file_path, 'easyocr')\n",
        "            if len(image_text) < 50:\n",
        "                chart_info = extract_text_from_charts_and_graphs(file_path)\n",
        "                return f\"Chart/Graph Analysis: {json.dumps(chart_info)}\"\n",
        "            return image_text\n",
        "    except Exception as e:\n",
        "        return f\"Error processing file: {str(e)}\"\n",
        "    return \"Unsupported file type.\"\n",
        "\n",
        "def text_to_audio(text: str) -> str:\n",
        "    try:\n",
        "        if not text or not text.strip():\n",
        "            log_to_file(\"Empty text provided for audio conversion\", \"WARNING\")\n",
        "            return \"\"\n",
        "\n",
        "        cleaned_text = text.strip()\n",
        "\n",
        "        tts = gTTS(text=cleaned_text, lang='en')\n",
        "        audio_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
        "        tts.save(audio_file.name)\n",
        "\n",
        "        log_to_file(\"Successfully converted text to audio\", \"INFO\")\n",
        "        return audio_file.name\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error converting text to audio: {str(e)}\"\n",
        "        log_to_file(error_msg, \"ERROR\")\n",
        "        return \"\"\n",
        "\n",
        "def log_to_file(message: str, level: str = \"INFO\") -> None:\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    log_message = f\"[{timestamp}] [{level}] {message}\"\n",
        "    with open('app.log', 'a+', encoding='utf-8') as log_file:\n",
        "        log_file.write(f\"{log_message}\\n\")\n",
        "        log_file.flush()\n",
        "\n",
        "def save_notes(notes: str, filename: str = \"notes.json\") -> Tuple[bool, str]:\n",
        "    try:\n",
        "        if not notes or not isinstance(notes, str):\n",
        "            log_to_file(\"Invalid notes format\", \"ERROR\")\n",
        "            return False, \"Invalid notes format\"\n",
        "        os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)\n",
        "        existing_notes = []\n",
        "        if os.path.exists(filename):\n",
        "            with open(filename, 'r', encoding='utf-8') as f:\n",
        "                existing_notes = json.load(f)\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        note_entry = {\"timestamp\": timestamp, \"content\": notes}\n",
        "        existing_notes.append(note_entry)\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(existing_notes, f, indent=4, ensure_ascii=False)\n",
        "        log_to_file(\"Notes saved successfully\", \"INFO\")\n",
        "        return True, \"Notes saved successfully!\"\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error saving notes: {str(e)}\"\n",
        "        log_to_file(error_msg, \"ERROR\")\n",
        "        return False, error_msg\n",
        "\n",
        "def safe_file_processing(file_path: str) -> Dict:\n",
        "    try:\n",
        "        if os.path.isdir(file_path):\n",
        "            log_to_file(f\"Skipping directory: {file_path}\", \"WARNING\")\n",
        "            return {\"error\": \"Directory paths are not supported\"}\n",
        "\n",
        "        if file_path and os.path.isfile(file_path):\n",
        "            file_info = {\n",
        "                \"name\": os.path.basename(file_path),\n",
        "                \"size\": os.path.getsize(file_path),\n",
        "                \"path\": file_path\n",
        "            }\n",
        "            log_to_file(f\"Successfully processed file: {file_path}\")\n",
        "            return file_info\n",
        "        log_to_file(f\"File not found: {file_path}\", \"WARNING\")\n",
        "        return {\"error\": f\"File not found: {file_path}\"}\n",
        "    except Exception as e:\n",
        "        log_to_file(f\"Error processing file {file_path}: {str(e)}\", \"ERROR\")\n",
        "        return {\"error\": f\"Error processing file: {str(e)}\"}\n",
        "\n",
        "def update_file_list(files: List[str]) -> Tuple[List[Dict], gr.components.CheckboxGroup]:\n",
        "    log_to_file(f\"Updating file list with {len(files)} files\")\n",
        "    valid_files = [\n",
        "        safe_file_processing(file)\n",
        "        for file in files\n",
        "        if not os.path.isdir(file) and \"error\" not in safe_file_processing(file)\n",
        "    ]\n",
        "    log_to_file(f\"Valid files processed: {len(valid_files)}\")\n",
        "    return valid_files, gr.CheckboxGroup(\n",
        "        choices=[f[\"name\"] for f in valid_files],\n",
        "        value=[],\n",
        "        label=\"Select files to delete\"\n",
        "    )\n",
        "\n",
        "def add_more_files(new_files: List[str], existing_files: Optional[List[Dict]]) -> Tuple[List[Dict], gr.components.CheckboxGroup]:\n",
        "    log_to_file(f\"Adding {len(new_files)} new files to existing {len(existing_files) if existing_files else 0} files\")\n",
        "    if existing_files is None:\n",
        "        existing_files = []\n",
        "    upload_dir = \"uploaded_files\"\n",
        "    os.makedirs(upload_dir, exist_ok=True)\n",
        "    new_file_infos = []\n",
        "\n",
        "    for file_path in new_files:\n",
        "        # Explicitly check and skip directories\n",
        "        if os.path.isdir(file_path):\n",
        "            log_to_file(f\"Skipping directory: {file_path}\", \"WARNING\")\n",
        "            continue\n",
        "\n",
        "        file_name = os.path.basename(file_path)\n",
        "        destination_path = os.path.join(upload_dir, file_name)\n",
        "        try:\n",
        "            with open(file_path, 'rb') as src_file:\n",
        "                with open(destination_path, 'wb') as dest_file:\n",
        "                    dest_file.write(src_file.read())\n",
        "            new_file_infos.append(safe_file_processing(destination_path))\n",
        "        except Exception as e:\n",
        "            log_to_file(f\"Error copying file {file_path}: {str(e)}\", \"ERROR\")\n",
        "            continue\n",
        "\n",
        "    # Combine new files with existing files\n",
        "    all_files = existing_files + [f for f in new_file_infos if \"error\" not in f]\n",
        "    unique_files = {f[\"path\"]: f for f in all_files}.values()\n",
        "    log_to_file(f\"Total unique files after addition: {len(unique_files)}\")\n",
        "\n",
        "    # Process all files together\n",
        "    return update_file_list([f[\"path\"] for f in unique_files])\n",
        "\n",
        "def delete_selected_files(files_to_delete: List[str], all_files: List[Dict]) -> Tuple[List[Dict], gr.components.CheckboxGroup, str]:\n",
        "    log_to_file(f\"Attempting to delete {len(files_to_delete)} files\")\n",
        "    if not all_files:\n",
        "        log_to_file(\"No files available for deletion\", \"WARNING\")\n",
        "        return [], gr.CheckboxGroup(choices=[], value=[], label=\"Select files to delete\"), \"Please upload files to begin.\"\n",
        "    if not files_to_delete:\n",
        "        log_to_file(\"No files selected for deletion\", \"WARNING\")\n",
        "        return all_files, gr.CheckboxGroup(choices=[f[\"name\"] for f in all_files], value=[]), \"\"\n",
        "    remaining_files = []\n",
        "    deleted_count = 0\n",
        "    error_count = 0\n",
        "    for file_info in all_files:\n",
        "        if file_info[\"name\"] not in files_to_delete:\n",
        "            remaining_files.append(file_info)\n",
        "        else:\n",
        "            try:\n",
        "                if os.path.exists(file_info[\"path\"]):\n",
        "                    os.remove(file_info[\"path\"])\n",
        "                    deleted_count += 1\n",
        "                    log_to_file(f\"Successfully deleted file: {file_info['path']}\")\n",
        "                else:\n",
        "                    log_to_file(f\"File not found for deletion: {file_info['path']}\", \"WARNING\")\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                log_to_file(f\"Error deleting file {file_info['path']}: {str(e)}\", \"ERROR\")\n",
        "    log_to_file(f\"Deletion summary - Successful: {deleted_count}, Failed: {error_count}, Remaining: {len(remaining_files)}\")\n",
        "    message = \"Please upload files to begin.\" if not remaining_files else \"\"\n",
        "    return remaining_files, gr.CheckboxGroup(choices=[f[\"name\"] for f in remaining_files], value=[]), message\n",
        "\n",
        "MODELS = {\n",
        "    'gemini-1.5-pro': genai.GenerativeModel('gemini-1.5-pro'),\n",
        "    'gemini-1.5-flash-8b': genai.GenerativeModel('gemini-1.5-flash-8b'),\n",
        "    'gemini-1.5-flash': genai.GenerativeModel('gemini-1.5-flash')\n",
        "}\n",
        "MODEL_OPTIONS = list(MODELS.keys())\n",
        "\n",
        "def generate_suggested_questions(content: str, model_name: str = 'gemini-1.5-pro') -> List[str]:\n",
        "    prompt = (f\"Based on the content provided in the following document, please suggest up to 3 clear and simple questions \"\n",
        "              f\"that are directly and easily answerable using information only from the document. \"\n",
        "              f\"Ensure that each question is specific to a fact, section, or topic explicitly mentioned in the document \"\n",
        "              f\"and that the answers are found directly in the text. Avoid generating any vague, complex, or abstract \"\n",
        "              f\"questions that require external information or inference beyond what's stated in the document. \"\n",
        "              f\"Each question must have a clear, direct answer based only on the document's content. \"\n",
        "              f\"If the document does not contain enough information to generate 3 questions, generate as many as possible \"\n",
        "              f\"that meet the criteria. \"\n",
        "              f\"Additionally, follow these guidelines: \"\n",
        "              f\"1. Identify key sections and topics within the document to ensure questions cover a range of content. \"\n",
        "              f\"2. Focus on factual information, dates, names, definitions, and specific details mentioned in the document. \"\n",
        "              f\"3. Avoid questions that require interpretation, opinion, or synthesis of information beyond the document. \"\n",
        "              f\"4. Ensure each question is concise and can be answered in one or two sentences. \"\n",
        "              f\"5. Cross-check each question to confirm that the answer is explicitly stated in the document. \"\n",
        "              f\"6. Prioritize questions that highlight the most important and relevant information in the document. \"\n",
        "              f\"7. If a section of the document is particularly dense with information, consider generating multiple questions from that section and don't give answers in front of questions generated. \"\n",
        "              f\"{content[:10000000]} \"\n",
        "              f\"Make sure the questions are straightforward and factual, and can be easily answered by referring to the content.\")\n",
        "    response = MODELS[model_name].generate_content(prompt)\n",
        "    return [question.strip() for question in response.text.split('\\n') if question.strip()][:3]\n",
        "\n",
        "def generate_combined_summary(docs: List[str], model_name: str = 'gemini-1.5-pro') -> Tuple[str, str]:\n",
        "    all_content = \"\"\n",
        "    for doc in docs:\n",
        "        # Extract the file path from the dictionary\n",
        "        file_path = doc['path'] if isinstance(doc, dict) else doc\n",
        "        file_content = process_file(file_path)  # Ensure file_path is a string\n",
        "        if not file_content.startswith(\"Error\"):\n",
        "            all_content += f\"Document: {os.path.basename(file_path)}\\n{file_content}\\n\\n\"\n",
        "    if not all_content:\n",
        "        return \"No valid documents to summarize.\", \"\"\n",
        "    summary_prompt = f\"Provide a concise summary of the following content from multiple documents:\\n{all_content}\"\n",
        "    combined_summary = MODELS[model_name].generate_content(summary_prompt).text\n",
        "    audio_file = text_to_audio(combined_summary)\n",
        "    return combined_summary, audio_file\n",
        "\n",
        "def summarize_and_suggest_questions(history: List, docs: List[str], stored_doc: List[str], model_name: str = 'gemini-1.5-pro') -> Tuple[List, List[str], List[str]]:\n",
        "    if not docs:\n",
        "        return history, stored_doc, [\"Please upload documents to generate questions.\"]\n",
        "    document_suggested_questions = []\n",
        "    combined_summary, combined_audio = generate_combined_summary(docs, model_name)\n",
        "    history.append([\"Combined Summary of All Documents:\", combined_summary])\n",
        "    history.append([\"\", gr.Audio(value=combined_audio, label=\"Audio for Combined Summary\")])\n",
        "    for doc in docs:\n",
        "        file_content = process_file(doc['path'])\n",
        "        if not file_content.startswith(\"Error\"):\n",
        "            stored_doc.append(file_content)\n",
        "            file_name = os.path.basename(doc['path'])\n",
        "            summary_prompt = f\"Provide a short summary of the following content:\\n{file_content}\"\n",
        "            summary = MODELS[model_name].generate_content(summary_prompt).text\n",
        "            audio_file = text_to_audio(summary)\n",
        "            history.append([f\"Summary for {file_name}:\", summary])\n",
        "            history.append([\"\", gr.Audio(value=audio_file, label=f\"Audio Summary for {file_name}\")])\n",
        "            suggested_questions = generate_suggested_questions(file_content, model_name)\n",
        "            document_suggested_questions.extend(suggested_questions)\n",
        "        else:\n",
        "            history.append([f\"Error processing {os.path.basename(doc['path'])}:\", file_content])\n",
        "    return history, stored_doc, document_suggested_questions\n",
        "\n",
        "def update_csv(serial_num: int, doc_name: str, doc_type: str, question: str, answer: str, csv_file_path: str = 'qa_log.csv') -> None:\n",
        "    file_exists = os.path.isfile(csv_file_path)\n",
        "    with open(csv_file_path, mode='a', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow([\"Serial Number\", \"Document Name\", \"Type of Document\", \"Question\", \"Answer\"])\n",
        "        writer.writerow([serial_num, doc_name, doc_type, question, answer])\n",
        "\n",
        "def save_current_chat(chat_history: List) -> str:\n",
        "    success, message = save_notes_to_document(chat_history_to_string(chat_history))\n",
        "    return message\n",
        "\n",
        "def chat_history_to_string(chat_history: List) -> str:\n",
        "    chat_string = \"\"\n",
        "    for i, (user_msg, bot_msg) in enumerate(chat_history):\n",
        "        chat_string += f\"Q: {user_msg}\\nA: {bot_msg}\\n\"\n",
        "        if i < len(chat_history) - 1:\n",
        "            chat_string += \"---\\n\"\n",
        "    return chat_string\n",
        "\n",
        "def save_notes_to_document(notes: str, filename: str = \"chat_history.docx\") -> Tuple[bool, str]:\n",
        "    try:\n",
        "        doc = docx.Document(filename) if os.path.exists(filename) else docx.Document()\n",
        "        doc.add_heading(f\"Chat History from {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", level=1)\n",
        "        for line in notes.split('\\n'):\n",
        "            if line.startswith('Q:'):\n",
        "                p = doc.add_paragraph()\n",
        "                p.add_run('Question: ').bold = True\n",
        "                p.add_run(line[2:].strip())\n",
        "            elif line.startswith('A:'):\n",
        "                p = doc.add_paragraph()\n",
        "                p.add_run('Answer: ').bold = True\n",
        "                p.add_run(line[2:].strip())\n",
        "            elif line.strip():\n",
        "                doc.add_paragraph(line)\n",
        "        doc.add_paragraph('---')\n",
        "        doc.save(filename)\n",
        "        log_to_file(f\"Chat history saved to document: {filename}\", \"INFO\")\n",
        "        return True, \"Chat saved successfully!\"\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error saving chat history to document: {str(e)}\"\n",
        "        log_to_file(error_msg, \"ERROR\")\n",
        "        return False, \"Chat not saved successfully.\"\n",
        "\n",
        "def find_relevant_passages(content: str, query: str, response: str, file_type: str) -> List[Dict]:\n",
        "    query_terms = set(query.lower().split())\n",
        "    response_terms = set(response.lower().split())\n",
        "    search_terms = query_terms.union(response_terms)\n",
        "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'is', 'are', 'was', 'were'}\n",
        "    search_terms = {term for term in search_terms if term not in stop_words}\n",
        "    chunks = []\n",
        "\n",
        "    if file_type == 'pdf':\n",
        "        pages = content.split('[PAGE ')\n",
        "        for page in pages[1:]:\n",
        "            try:\n",
        "                page_num = int(page.split(']')[0])\n",
        "                page_text = page.split(']')[1]\n",
        "                sentences = [s.strip() + '.' for s in page_text.split('.') if s.strip()]\n",
        "\n",
        "                for sentence in sentences:\n",
        "                    term_matches = sum(1 for term in search_terms if term in sentence.lower())\n",
        "                    if term_matches > 1 or any(\n",
        "                        phrase.lower() in sentence.lower()\n",
        "                        for phrase in response.split('.')\n",
        "                        if len(phrase.split()) > 2\n",
        "                    ):\n",
        "                        chunks.append({\n",
        "                            'identifier': f\"page_{page_num}\",\n",
        "                            'text': sentence,\n",
        "                            'relevance': term_matches,\n",
        "                            'type': 'pdf'\n",
        "                        })\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    elif file_type in ['csv', 'xlsx', 'xls']:\n",
        "        sections = content.split('Complete Data:')[1].split('Basic Statistics:')[0] if 'Complete Data:' in content else content\n",
        "        rows = sections.split('[ROW ')\n",
        "        for row in rows[1:]:\n",
        "            try:\n",
        "                row_num = int(row.split(']')[0])\n",
        "                row_text = row.split(']')[1].strip()\n",
        "                term_matches = sum(1 for term in search_terms if term in row_text.lower())\n",
        "\n",
        "                if term_matches > 0:\n",
        "                    chunks.append({\n",
        "                        'identifier': f\"row_{row_num}\",\n",
        "                        'text': row_text,\n",
        "                        'relevance': term_matches,\n",
        "                        'type': 'spreadsheet'\n",
        "                    })\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    elif file_type == 'pptx':\n",
        "        slides = content.split('[SLIDE ')\n",
        "        for slide in slides[1:]:\n",
        "            try:\n",
        "                slide_num = int(slide.split(']')[0])\n",
        "                slide_text = slide.split(']')[1]\n",
        "                term_matches = sum(1 for term in search_terms if term in slide_text.lower())\n",
        "\n",
        "                if term_matches > 1:\n",
        "                    chunks.append({\n",
        "                        'identifier': f\"slide_{slide_num}\",\n",
        "                        'text': slide_text,\n",
        "                        'relevance': term_matches,\n",
        "                        'type': 'pptx'\n",
        "                    })\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    elif file_type in ['txt', 'docx']:\n",
        "        paragraphs = content.split('\\n')\n",
        "        current_section = []\n",
        "        section_num = 1\n",
        "\n",
        "        for i, paragraph in enumerate(paragraphs):\n",
        "            if paragraph.strip():\n",
        "                # Add paragraph markers for better navigation\n",
        "                marked_paragraph = f'<div id=\"para_{i+1}\" class=\"paragraph-content\">{paragraph}</div>'\n",
        "                current_section.append(marked_paragraph)\n",
        "\n",
        "                if len(current_section) >= 3 or len(paragraph) > 200:\n",
        "                    section_text = ' '.join(current_section)\n",
        "                    term_matches = sum(1 for term in search_terms if term.lower() in section_text.lower())\n",
        "\n",
        "                    # Check for exact phrase matches from the response\n",
        "                    response_phrases = [phrase.strip() for phrase in response.split('.') if len(phrase.strip().split()) > 2]\n",
        "                    phrase_matches = sum(1 for phrase in response_phrases if phrase.lower() in section_text.lower())\n",
        "\n",
        "                    relevance_score = term_matches + (phrase_matches * 2)  # Weight phrase matches more heavily\n",
        "\n",
        "                    if relevance_score > 0:\n",
        "                        chunks.append({\n",
        "                            'identifier': f\"para_{i-len(current_section)+1}\",\n",
        "                            'text': section_text,\n",
        "                            'relevance': relevance_score,\n",
        "                            'type': 'document',\n",
        "                            'paragraph_number': i-len(current_section)+1\n",
        "                        })\n",
        "                    section_num += 1\n",
        "                    current_section = []\n",
        "\n",
        "    chunks.sort(key=lambda x: x['relevance'], reverse=True)\n",
        "    return chunks[:3]\n",
        "\n",
        "HIGHLIGHT_COLORS = [\n",
        "    \"rgba(255, 255, 0, 0.3)\",  # Yellow\n",
        "    \"rgba(173, 216, 230, 0.3)\",  # Light Blue\n",
        "    \"rgba(255, 165, 0, 0.3)\",  # Orange\n",
        "    \"rgba(144, 238, 144, 0.3)\",  # Light Green\n",
        "    \"rgba(255, 192, 203, 0.3)\"   # Light Pink\n",
        "]\n",
        "\n",
        "def llm_response(history: List, text: str, docs: List[str], stored_doc: List[str], serial_num: int, model_name: str = 'gemini-1.5-pro') -> Tuple[List, List[str], int, str, str]:\n",
        "    if not text.strip():\n",
        "        return history, [], serial_num, \"\", \"\"\n",
        "\n",
        "    highlighted_content = \"\"\n",
        "    source_chunks_html = []\n",
        "    docs_response = None\n",
        "\n",
        "    if docs:\n",
        "        all_docs_content = {}\n",
        "        for doc in docs:\n",
        "            try:\n",
        "                file_ext = os.path.splitext(doc['path'])[1].lower()\n",
        "                file_content = process_file(doc['path'])\n",
        "                if not file_content.startswith(\"Error\"):\n",
        "                    # Add paragraph markers for DOCX and TXT files\n",
        "                    if file_ext in ['.txt', '.docx']:\n",
        "                        paragraphs = file_content.split('\\n')\n",
        "                        marked_content = []\n",
        "                        for i, para in enumerate(paragraphs, 1):\n",
        "                            if para.strip():\n",
        "                                marked_content.append(f'<div id=\"para_{i}\" class=\"paragraph-content\">{para}</div>')\n",
        "                        file_content = '\\n'.join(marked_content)\n",
        "\n",
        "                    all_docs_content[doc['path']] = {\n",
        "                        'content': file_content,\n",
        "                        'type': file_ext[1:]  # Remove the dot from extension\n",
        "                    }\n",
        "            except Exception as e:\n",
        "                log_to_file(f\"Error processing file {doc['path']}: {str(e)}\", \"ERROR\")\n",
        "                continue\n",
        "\n",
        "        if all_docs_content:\n",
        "            # Determine the most relevant document\n",
        "            relevance_prompt = f\"\"\"\n",
        "            From these documents, which one is most likely to contain the answer to this question?\n",
        "            Question: {text}\n",
        "            Documents:\n",
        "            {chr(10).join(f\"{os.path.basename(doc)}: {info['content'][:500]}\" for doc, info in all_docs_content.items())}\n",
        "\n",
        "            Respond with ONLY the filename of the most relevant document.\n",
        "            \"\"\"\n",
        "\n",
        "            most_relevant_doc = MODELS[model_name].generate_content(relevance_prompt).text.strip()\n",
        "\n",
        "            # Find the full path of the most relevant document\n",
        "            most_relevant_path = next(\n",
        "                (doc for doc in all_docs_content if os.path.basename(doc) == most_relevant_doc),\n",
        "                list(all_docs_content.keys())[0]  # fallback to first document if no match\n",
        "            )\n",
        "\n",
        "            doc_info = all_docs_content[most_relevant_path]\n",
        "\n",
        "            context_check = MODELS[model_name].generate_content(\n",
        "                f\"Based on this document content, can the following question be fully answered using only this information? Reply with just 'yes' or 'no'.\\n\\nDocument: {doc_info['content']}\\n\\nQuestion: {text}\"\n",
        "            ).text.lower().strip()\n",
        "\n",
        "            if context_check == 'yes':\n",
        "                docs_response = MODELS[model_name].generate_content(\n",
        "                    f\"Answer this question using only the provided document content. Also specify which document type ({doc_info['type'].upper()}) contains the main answer. Include specific details and quotes when possible.\\n\\nDocument: {doc_info['content']}\\n\\nQuestion: {text}\"\n",
        "                ).text\n",
        "\n",
        "                chunks = find_relevant_passages(\n",
        "                    doc_info['content'],\n",
        "                    text,\n",
        "                    docs_response,\n",
        "                    doc_info['type']\n",
        "                )\n",
        "\n",
        "                if chunks:\n",
        "                    doc_name = os.path.basename(most_relevant_path)\n",
        "                    highlighted_content += f'<div class=\"source-guide\"><h4>{doc_name}</h4>'\n",
        "                    highlighted_text = doc_info['content']\n",
        "\n",
        "                    for chunk in chunks:\n",
        "                        color = random.choice(HIGHLIGHT_COLORS)\n",
        "                        identifier = chunk['identifier']\n",
        "                        chunk_text = chunk['text']\n",
        "\n",
        "                        # Format source reference based on file type\n",
        "                        if chunk['type'] == 'pdf':\n",
        "                            page_num = identifier.split('_')[1]\n",
        "                            source_ref = f'[PDF Page {page_num}]'\n",
        "                        elif chunk['type'] in ['csv', 'xlsx', 'xls']:\n",
        "                            row_num = identifier.split('_')[1]\n",
        "                            source_ref = f'[Row {row_num}]'\n",
        "                        elif chunk['type'] == 'pptx':\n",
        "                            slide_num = identifier.split('_')[1]\n",
        "                            source_ref = f'[Slide {slide_num}]'\n",
        "                        else:\n",
        "                            section_num = identifier.split('_')[1]\n",
        "                            source_ref = f'[Section {section_num}]'\n",
        "\n",
        "                        source_chunks_html.append(\n",
        "                            f'<a href=\"#{identifier}\" class=\"chunk-link\">{source_ref}</a>'\n",
        "                        )\n",
        "\n",
        "                        highlighted_chunk = (\n",
        "                            f'<div id=\"{identifier}\">'\n",
        "                            f'<span class=\"highlight\" style=\"background-color: {color}\">'\n",
        "                            f'{chunk_text}</span>'\n",
        "                            f'</div>'\n",
        "                        )\n",
        "                        highlighted_text = highlighted_text.replace(chunk_text, highlighted_chunk)\n",
        "\n",
        "                    highlighted_content += f\"\"\"\n",
        "                        <div class=\"source-content\" id=\"scrollable-content\">\n",
        "                            {highlighted_text}\n",
        "                        </div>\n",
        "                    </div>\n",
        "                    \"\"\"\n",
        "\n",
        "    response_text = docs_response or \"I couldn't find a relevant answer in the provided documents.\"\n",
        "    if source_chunks_html:\n",
        "        response_text += \"\\n\\nSources: \" + \" \".join(source_chunks_html)\n",
        "\n",
        "    response_audio = text_to_audio(response_text)\n",
        "    history.append([text, response_text])\n",
        "    history.append([\"\", gr.Audio(value=response_audio, label=\"Audio Response\")])\n",
        "\n",
        "    new_suggested_questions = []\n",
        "    if docs:\n",
        "        for doc in docs:\n",
        "            file_content = process_file(doc['path'])\n",
        "            suggested_questions = generate_suggested_questions(file_content, model_name)\n",
        "            new_suggested_questions.extend(suggested_questions)\n",
        "            if docs_response:\n",
        "                doc_name = os.path.basename(doc['path'])\n",
        "                doc_type = os.path.splitext(doc_name)[1].lower()\n",
        "                update_csv(serial_num, doc_name, doc_type, text, response_text)\n",
        "\n",
        "    serial_num += 1\n",
        "    save_current_chat(history)\n",
        "    return history, new_suggested_questions, serial_num, \"\", highlighted_content\n",
        "\n",
        "def safe_file_upload(files):\n",
        "    \"\"\"\n",
        "    Safely process uploaded files, filtering out directories and handling potential upload issues.\n",
        "    \"\"\"\n",
        "    processed_files = []\n",
        "    upload_dir = \"uploaded_files\"\n",
        "    os.makedirs(upload_dir, exist_ok=True)\n",
        "\n",
        "    # Ensure files is a list\n",
        "    if not isinstance(files, list):\n",
        "        files = [files]\n",
        "\n",
        "    for file_path in files:\n",
        "        try:\n",
        "            # Use os.path.isdir to check for directories\n",
        "            if os.path.isdir(file_path):\n",
        "                log_to_file(f\"Skipping directory: {file_path}\", \"WARNING\")\n",
        "                continue\n",
        "\n",
        "            # Use os.path.isfile to ensure it's a file\n",
        "            if not os.path.isfile(file_path):\n",
        "                log_to_file(f\"Not a valid file: {file_path}\", \"WARNING\")\n",
        "                continue\n",
        "\n",
        "            # Generate a unique filename to prevent overwrites\n",
        "            file_name = os.path.basename(file_path)\n",
        "            unique_filename = f\"{uuid.uuid4()}_{file_name}\"\n",
        "            destination_path = os.path.join(upload_dir, unique_filename)\n",
        "\n",
        "            # Copy file to upload directory\n",
        "            shutil.copy2(file_path, destination_path)\n",
        "            processed_files.append(destination_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            log_to_file(f\"Error processing file {file_path}: {str(e)}\", \"ERROR\")\n",
        "\n",
        "    return processed_files\n",
        "\n",
        "def create_synthlinx_app() -> gr.Blocks:\n",
        "    edit_message_script = \"\"\"\n",
        "    function editMessage(button) {\n",
        "        const message = button.getAttribute('data-message');\n",
        "        const textbox = document.querySelector('textarea');\n",
        "        textbox.value = message;\n",
        "        textbox.focus();\n",
        "    }\n",
        "    \"\"\"\n",
        "    custom_css = \"\"\"\n",
        "    #header {\n",
        "        display: flex;\n",
        "        align-items: center;\n",
        "        padding: 10px;\n",
        "        background: white;\n",
        "        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
        "        position: fixed;\n",
        "        top: 0;\n",
        "        left: 45px;\n",
        "        right: 0;\n",
        "        z-index: 1002;\n",
        "        height: 60px;\n",
        "    }\n",
        "    #logo {\n",
        "        height: 40px;\n",
        "        margin-right: 15px;\n",
        "    }\n",
        "    #header h1 {\n",
        "        margin: 0;\n",
        "        color: #333;\n",
        "        font-size: 24px;\n",
        "    }\n",
        "    #file-management-panel {\n",
        "        position: fixed;\n",
        "        left: -300px;\n",
        "        top: 100px;\n",
        "        width: 300px;\n",
        "        height: calc(100vh - 60px);\n",
        "        background: white;\n",
        "        transition: left 0.3s ease;\n",
        "        box-shadow: 2px 0 5px rgba(0,0,0,0.1);\n",
        "        z-index: 1000;\n",
        "    }\n",
        "    #file-management-panel.active {\n",
        "        left: 0;\n",
        "    }\n",
        "    .menu-toggle {\n",
        "        position: fixed;\n",
        "        left: 0;\n",
        "        top: 0;\n",
        "        z-index: 1003;\n",
        "        background: #f0f0f0;\n",
        "        border: none;\n",
        "        cursor: pointer;\n",
        "        padding: 10px;\n",
        "        margin: 0;\n",
        "        width: 45px;\n",
        "        height: 60px;\n",
        "        display: flex;\n",
        "        flex-direction: column;\n",
        "        justify-content: center;\n",
        "        align-items: center;\n",
        "    }\n",
        "    .menu-toggle span {\n",
        "        display: block;\n",
        "        width: 25px;\n",
        "        height: 3px;\n",
        "        background: #333;\n",
        "        margin: 2px 0;\n",
        "        transition: 0.3s;\n",
        "    }\n",
        "    .source-guide {\n",
        "        border: 1px solid #e0e0e0;\n",
        "        border-radius: 8px;\n",
        "        padding: 16px;\n",
        "        margin: 8px 0;\n",
        "        background: #ffffff;\n",
        "    }\n",
        "    .source-content {\n",
        "        max-height: 500px;\n",
        "        overflow-y: auto;\n",
        "        font-family: system-ui;\n",
        "        line-height: 1.5;\n",
        "    }\n",
        "    .highlight {\n",
        "        background-color: rgba(255, 255, 0, 0.3);\n",
        "        padding: 2px 4px;\n",
        "        border-radius: 3px;\n",
        "    }\n",
        "    .chunk-link {\n",
        "        background-color: #f0f0f0;\n",
        "        color: #333;\n",
        "        padding: 2px 6px;\n",
        "        border-radius: 3px;\n",
        "        text-decoration: none;\n",
        "        margin-right: 5px;\n",
        "        cursor: pointer;\n",
        "    }\n",
        "    .chunk-link:hover {\n",
        "        background-color: #e0e0e0;\n",
        "    }\n",
        "    #scrollable-content {\n",
        "        scroll-behavior: smooth;\n",
        "    }\n",
        "    #main-content {\n",
        "        margin-left: 0;\n",
        "        margin-top: 20px;\n",
        "        transition: margin-left 0.3s ease;\n",
        "        padding: 20px;\n",
        "        position: relative;\n",
        "        z-index: 999;\n",
        "    }\n",
        "    .chatbot-container {\n",
        "    margin-top: 10px;\n",
        "    padding-top: 10px;\n",
        "    }\n",
        "    #main-content.shifted {\n",
        "        margin-left: 300px;\n",
        "    }\n",
        "    #file-management-panel {\n",
        "        position: fixed;\n",
        "        left: -300px;\n",
        "        top: 60px;\n",
        "        width: 300px;\n",
        "        height: calc(100vh - 60px);\n",
        "        background: white;\n",
        "        transition: left 0.3s ease;\n",
        "        box-shadow: 2px 0 5px rgba(0,0,0,0.1);\n",
        "        z-index: 1000;\n",
        "    }\n",
        "    #file-management-panel .file-content {\n",
        "        height: 100%;\n",
        "        overflow-y: auto;\n",
        "        padding: 20px 15px;\n",
        "    }\n",
        "    #file-management-panel .upload-section,\n",
        "    #file-management-panel .delete-section {\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "    #file-management-panel .gr-file {\n",
        "        margin-bottom: 10px;\n",
        "    }\n",
        "    #file-management-panel .gr-form {\n",
        "        max-height: calc(100vh - 250px);\n",
        "        overflow-y: auto;\n",
        "    }\n",
        "    #file-management-panel > div {\n",
        "        padding: 0 15px;\n",
        "        width: 100%;\n",
        "    }\n",
        "    #file-management-panel .gr-form {\n",
        "        display: flex;\n",
        "        flex-direction: column;\n",
        "        gap: 10px;\n",
        "        margin-top: 10px;\n",
        "    }\n",
        "\n",
        "    #file-management-panel .gr-check-radio {\n",
        "        margin-bottom: 8px;\n",
        "    }\n",
        "\n",
        "    .interaction-icons {\n",
        "        display: flex;\n",
        "        gap: 12px;\n",
        "        padding: 8px 0;\n",
        "        align-items: center;\n",
        "        position: absolute;\n",
        "        right: 8px;\n",
        "        bottom: 0;\n",
        "    }\n",
        "\n",
        "    .icon-button {\n",
        "        background: none;\n",
        "        border: none;\n",
        "        cursor: pointer;\n",
        "        padding: 6px;\n",
        "        border-radius: 4px;\n",
        "        transition: background-color 0.2s;\n",
        "    }\n",
        "\n",
        "    .icon-button:hover {\n",
        "        background-color: #f0f0f0;\n",
        "    }\n",
        "\n",
        "    .icon-button img {\n",
        "        width: 20px;\n",
        "        height: 20px;\n",
        "        opacity: 0.7;\n",
        "    }\n",
        "\n",
        "    .icon-button:hover img {\n",
        "        opacity: 1;\n",
        "    }\n",
        "\n",
        "    .message-container {\n",
        "        position: relative;\n",
        "        width: 100%;\n",
        "    }\n",
        "\n",
        "    .edit-message {\n",
        "        font-size: 14px;\n",
        "        padding: 4px 8px;\n",
        "        background: none;\n",
        "        border: none;\n",
        "        cursor: pointer;\n",
        "        opacity: 0.7;\n",
        "    }\n",
        "\n",
        "    .edit-message:hover {\n",
        "        opacity: 1;\n",
        "    }\n",
        "\n",
        "    .highlight-section {\n",
        "        margin: 10px 0;\n",
        "        padding: 10px;\n",
        "        border-left: 3px solid #007bff;\n",
        "    }\n",
        "    .highlight {\n",
        "        background-color: rgba(255, 255, 0, 0.3);\n",
        "        padding: 2px 4px;\n",
        "        border-radius: 3px;\n",
        "    }\n",
        "    .chunk-link {\n",
        "        background-color: #f0f0f0;\n",
        "        color: #333;\n",
        "        padding: 2px 6px;\n",
        "        border-radius: 3px;\n",
        "        text-decoration: none;\n",
        "        margin-right: 5px;\n",
        "        cursor: pointer;\n",
        "    }\n",
        "    .chunk-link:hover {\n",
        "        background-color: #e0e0e0;\n",
        "    }\n",
        "    #scrollable-content {\n",
        "        scroll-behavior: smooth;\n",
        "        max-height: 300px;\n",
        "        overflow-y: auto;\n",
        "    }\n",
        "    .page-content {\n",
        "        scroll-margin-top: 60px;\n",
        "        margin-bottom: 20px;\n",
        "        padding: 10px;\n",
        "        border-bottom: 1px solid #eee;\n",
        "    }\n",
        "\n",
        "    .highlight {\n",
        "        background-color: rgba(255, 255, 0, 0.3);\n",
        "        padding: 2px 4px;\n",
        "        border-radius: 3px;\n",
        "    }\n",
        "\n",
        "    .chunk-link {\n",
        "        display: inline-block;\n",
        "        background-color: #f0f0f0;\n",
        "        color: #333;\n",
        "        padding: 2px 6px;\n",
        "        border-radius: 3px;\n",
        "        text-decoration: none;\n",
        "        margin-right: 5px;\n",
        "        margin-bottom: 5px;\n",
        "        cursor: pointer;\n",
        "    }\n",
        "\n",
        "    .chunk-link:hover {\n",
        "        background-color: #e0e0e0;\n",
        "    }\n",
        "\n",
        "    #scrollable-content {\n",
        "        scroll-behavior: smooth;\n",
        "        max-height: 500px;\n",
        "        overflow-y: auto;\n",
        "        padding: 10px;\n",
        "    }\n",
        "    .upload-section .file-upload {\n",
        "        border: 2px dashed #ccc;\n",
        "        border-radius: 4px;\n",
        "        padding: 20px;\n",
        "        text-align: center;\n",
        "        margin-bottom: 15px;\n",
        "    }\n",
        "\n",
        "    .upload-section .file-upload.invalid {\n",
        "        border-color: #ff4444;\n",
        "    }\n",
        "\n",
        "    .upload-section .file-error {\n",
        "        color: #ff4444;\n",
        "        font-size: 0.9em;\n",
        "        margin-top: 5px;\n",
        "    }\n",
        "    .paragraph-content {\n",
        "        margin: 10px 0;\n",
        "        padding: 10px;\n",
        "        border-radius: 4px;\n",
        "        scroll-margin-top: 70px;\n",
        "    }\n",
        "\n",
        "    .paragraph-content.highlighted {\n",
        "        background-color: rgba(255, 255, 0, 0.3);\n",
        "        border-left: 3px solid #007bff;\n",
        "    }\n",
        "\n",
        "    .document-source-link {\n",
        "        display: inline-block;\n",
        "        background-color: #f0f0f0;\n",
        "        color: #333;\n",
        "        padding: 4px 8px;\n",
        "        border-radius: 4px;\n",
        "        text-decoration: none;\n",
        "        margin: 2px 4px;\n",
        "        font-size: 0.9em;\n",
        "        cursor: pointer;\n",
        "    }\n",
        "\n",
        "    .document-source-link:hover {\n",
        "        background-color: #e0e0e0;\n",
        "        text-decoration: none;\n",
        "    }\n",
        "    \"\"\"\n",
        "    with gr.Blocks(css=custom_css) as app:\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            <div id=\"header\">\n",
        "                <img id=\"logo\" src=\"https://media.licdn.com/dms/image/v2/D560BAQGF6NHHgadKYg/company-logo_200_200/company-logo_200_200/0/1707117426803?e=1738800000&v=beta&t=X8bdl_4FhyhYPECVAfvKiQoKT5Ofe-K7PtcPLper0nE\" alt=\"Company Logo\">\n",
        "                <h1>Synthlinx.AI</h1>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        )\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                toggle_button = gr.Button(\"☰\", elem_classes=[\"menu-toggle\"])\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(elem_id=\"file-management-panel\"):\n",
        "                with gr.Column(elem_classes=\"file-content\"):\n",
        "                    gr.Markdown(\"### File Management\")\n",
        "\n",
        "                    with gr.Column(elem_classes=\"upload-section\"):\n",
        "                        file_list = gr.State([])\n",
        "                        file_display = gr.File(\n",
        "                            label=\"Uploaded Documents\",\n",
        "                            file_count=\"multiple\",\n",
        "                            type=\"filepath\",\n",
        "                            file_types=[\n",
        "                                \".pdf\", \".docx\", \".txt\", \".csv\",\n",
        "                                \".xlsx\", \".xls\", \".pptx\",\n",
        "                                \".png\", \".jpg\", \".jpeg\", \".tiff\",\n",
        "                                \".bmp\", \".webp\"\n",
        "                            ],\n",
        "                            interactive=False\n",
        "                        )\n",
        "                        add_files_button = gr.UploadButton(\n",
        "                            \"Add More Files\",\n",
        "                            file_count=\"multiple\",\n",
        "                            file_types=[\n",
        "                                \".pdf\", \".docx\", \".txt\", \".csv\",\n",
        "                                \".xlsx\", \".xls\", \".pptx\",\n",
        "                                \".png\", \".jpg\", \".jpeg\", \".tiff\",\n",
        "                                \".bmp\", \".webp\"\n",
        "                            ],\n",
        "                            type=\"filepath\"\n",
        "                        )\n",
        "\n",
        "                    with gr.Column(elem_classes=\"delete-section\"):\n",
        "                        files_to_delete = gr.CheckboxGroup(\n",
        "                            label=\"Select files to delete\",\n",
        "                            choices=[],\n",
        "                            interactive=True\n",
        "                        )\n",
        "                        delete_files_button = gr.Button(\"Delete Selected Files\")\n",
        "                        status_message = gr.Markdown(\"\")\n",
        "\n",
        "            with gr.Column(elem_id=\"main-content\"):\n",
        "                model_dropdown = gr.Dropdown(\n",
        "                    choices=MODEL_OPTIONS,\n",
        "                    value=MODEL_OPTIONS[0],\n",
        "                    label=\"Select Model\",\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "                chatbot = gr.Chatbot(\n",
        "                    render_markdown=True,\n",
        "                    bubble_full_width=False,\n",
        "                    show_copy_button=True,\n",
        "                    scale=2,\n",
        "                    height=550,\n",
        "                    elem_classes=\"chatbot-container\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=2):\n",
        "                        suggested_questions_radio = gr.Radio(\n",
        "                            label=\"Suggested Questions\",\n",
        "                            choices=[],\n",
        "                            interactive=True\n",
        "                        )\n",
        "                        text_box = gr.Textbox(\n",
        "                            placeholder=\"Enter your question and Click on Submit\",\n",
        "                            container=False\n",
        "                        )\n",
        "                        submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "                        source_chunks = gr.HTML(label=\"Source Chunks\")\n",
        "                        source_content = gr.HTML(label=\"Source Content\")\n",
        "\n",
        "                def handle_message_click(evt: gr.SelectData, chat_history):\n",
        "                    clicked_message = chat_history[evt.index[0]][0]\n",
        "                    return clicked_message\n",
        "\n",
        "                def handle_chat(message, history):\n",
        "                    response, audio = assistant.generate_response(message)\n",
        "                    source_results = []\n",
        "                    source_chunks_html = []\n",
        "                    for file_name in assistant.current_files:\n",
        "                        result = assistant.highlight_source_text(file_name, message)\n",
        "                        if result['chunks']:\n",
        "                            source_chunks_html.extend(result['chunks'])\n",
        "                            source_results.append(f\"[File: {file_name}]\\n{result['highlighted_text']}\")\n",
        "\n",
        "                    full_response = response + \"\\n\\n\" + \" \".join(source_chunks_html)\n",
        "\n",
        "                    history.append([message, full_response])\n",
        "                    history.append([\"\", gr.Audio(value=audio)])\n",
        "\n",
        "                    return (\n",
        "                        history,\n",
        "                        \"\",\n",
        "                        \"\",\n",
        "                        \"\\n\".join(source_results)\n",
        "                    )\n",
        "\n",
        "                chatbot.select(\n",
        "                    handle_message_click,\n",
        "                    inputs=[chatbot],\n",
        "                    outputs=[text_box]\n",
        "                )\n",
        "\n",
        "        toggle_script = \"\"\"\n",
        "        function togglePanel() {\n",
        "            const panel = document.getElementById('file-management-panel');\n",
        "            const mainContent = document.getElementById('main-content');\n",
        "            panel.classList.toggle('active');\n",
        "            mainContent.classList.toggle('shifted');\n",
        "            return [];\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        toggle_button.click(\n",
        "            fn=lambda: None,\n",
        "            inputs=[],\n",
        "            outputs=[],\n",
        "            js=toggle_script\n",
        "        )\n",
        "\n",
        "        stored_doc = gr.State([])\n",
        "        suggested_questions = gr.State([])\n",
        "        serial_num = gr.State(1)\n",
        "\n",
        "        add_files_button.upload(\n",
        "            safe_file_upload,\n",
        "            inputs=[add_files_button],\n",
        "            outputs=[file_display]\n",
        "        ).then(\n",
        "            add_more_files,\n",
        "            inputs=[file_display, file_list],\n",
        "            outputs=[file_list, files_to_delete]\n",
        "        ).then(\n",
        "            summarize_and_suggest_questions,\n",
        "            inputs=[chatbot, file_list, stored_doc, model_dropdown],\n",
        "            outputs=[chatbot, stored_doc, suggested_questions]\n",
        "        ).then(\n",
        "            lambda: \"\",\n",
        "            outputs=status_message\n",
        "        ).then(\n",
        "            lambda files: files if files else [],  # Ensure non-None input\n",
        "            inputs=[file_display],\n",
        "            outputs=[file_display]\n",
        "        )\n",
        "\n",
        "        delete_files_button.click(\n",
        "            delete_selected_files,\n",
        "            inputs=[files_to_delete, file_list],\n",
        "            outputs=[file_list, files_to_delete, status_message]\n",
        "        ).then(\n",
        "            lambda files: [f[\"path\"] for f in files] if files else [],\n",
        "            inputs=[file_list],\n",
        "            outputs=[file_display]\n",
        "        ).then(\n",
        "            lambda: [],\n",
        "            outputs=[chatbot]\n",
        "        ).then(\n",
        "            lambda: [],\n",
        "            outputs=[stored_doc]\n",
        "        ).then(\n",
        "            summarize_and_suggest_questions,\n",
        "            inputs=[chatbot, file_list, stored_doc, model_dropdown],\n",
        "            outputs=[chatbot, stored_doc, suggested_questions]\n",
        "        )\n",
        "\n",
        "        model_dropdown.change(\n",
        "            summarize_and_suggest_questions,\n",
        "            inputs=[chatbot, file_list, stored_doc, model_dropdown],\n",
        "            outputs=[chatbot, stored_doc, suggested_questions]\n",
        "        )\n",
        "\n",
        "        suggested_questions.change(\n",
        "            lambda q: gr.update(choices=q),\n",
        "            inputs=[suggested_questions],\n",
        "            outputs=[suggested_questions_radio]\n",
        "        )\n",
        "\n",
        "        suggested_questions_radio.change(\n",
        "            lambda choice: choice if choice else \"\",\n",
        "            inputs=[suggested_questions_radio],\n",
        "            outputs=[text_box]\n",
        "        )\n",
        "\n",
        "        submit_button.click(\n",
        "            llm_response,\n",
        "            inputs=[chatbot, text_box, file_list, stored_doc, serial_num, model_dropdown],\n",
        "            outputs=[chatbot, suggested_questions, serial_num, source_chunks, source_content]\n",
        "        ).then(\n",
        "            lambda: \"\",\n",
        "            outputs=[text_box]\n",
        "        )\n",
        "    return app\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = create_synthlinx_app()\n",
        "    app.launch(debug=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "sTer2HKGmRDM",
        "outputId": "f10906ca-2c18-44ac-dbe6-2c7ee2632ef9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:242: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e181cb75b1307b3949.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e181cb75b1307b3949.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e181cb75b1307b3949.gradio.live\n"
          ]
        }
      ]
    }
  ]
}